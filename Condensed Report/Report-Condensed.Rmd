---
title: "Condensed report"
author: "Max Austin - Mada2"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

#install.packages("tinytex")
#tinytex::install_tinytex()

#install.packages("readr")
library(readr)

#install.packages("dplyr")
library(dplyr)

#install.packages("tidyr")
library(tidyr)

#install.packages("stringr")
library(stringr)

#install.packages("ggplot2")
library(ggplot2)

#install.packages("viridis")
library(viridis)

#install.packages("treemap")
library(treemap)


#install.packages("corrplot")
library(corrplot)

#install.packages("cluster")
library(cluster)


#install.packages("factoextra")
library(factoextra)

#install.packages("knitr")
library(knitr)

#install.packages("igraph")
library(igraph)

#install.packages("ggraph")
library(ggraph)

#install.packages("tidygraph")
library(tidygraph)


#install.packages("reticulate")
#install.packages("tensorflow")
#install.packages("keras")
library(reticulate)
#reticulate::virtualenv_remove("r-tensorflow")
library(keras)
#keras::install_keras(method = "virtualenv", envname = "r-tensorflow")
#reticulate::use_virtualenv("r-tensorflow", required = TRUE)
library(keras)
library(tensorflow)
#reticulate::py_config()
#tf$constant("Hello TensorFlow!")


```
# Introduction

This paper analyzes the United Kingdom's international trade activity from 1995 to 2023 using data from the CEPII BACI database. The aim is to test whether future UK trade values—total trade, product-level flows, and country-level trade patterns—can be accurately forecasted using current data-based techniques.

To answer this, the report focuses on three significant questions: Will UK volumes of trade rise or fall in coming years? What goods and trading partners will set future trends? And what is difficult when attempting to forecast long-term trade?

The original data comprises over 258 million observations of trade between 238 nations and 5,000 products. It is reduced to UK trade only as an importer or exporter. The work is conducted in four stages: initial exploration and visualization, correlation analysis, product and country clustering, and finally, deep learning models for prediction.

Additional exploratory plots and additional figures are provided in the supplementary PowerPoint appendix due to space constraints.

# Exploration and Visualisation
```{r include=FALSE}
## Data import and processing
col_spec <- cols(
  k = col_character(),  
  t = col_double(),     
  i = col_double(),
  j = col_double(),
  v = col_double(),
  q = col_double()
)

files <- list.files(pattern = "*.csv")
data_list <- lapply(files[1:29], function(file) {
  read_csv(file, col_types = col_spec)
})
country_codes <- read_csv("codes/country_codes_V202501.csv")
product_codes <- read_csv("codes/product_codes_HS92_V202501.csv")  

rename_columns <- function(data) {
  data <- data %>%
    rename(
      year = t,          # 't' to 'year'
      exporter = i,      # 'i' to 'exporter'
      importer = j,      # 'j' to 'importer'
      product = k,       # 'k' to 'product'
      value = v,         # 'v' to 'value'
      quantity = q       # 'q' to 'quantity'
    )
  return(data)
}

data_list <- lapply(data_list, rename_columns)


combined_data <- bind_rows(data_list)
### 258605562 obs

rm(data_list)
gc()
combined_data <- combined_data %>%
  filter(exporter == "826" | importer == "826") 
### 13367259 obs

replace_country_codes <- function(data) {
  data <- data %>%
    mutate(exporter = as.character(exporter),
           importer = as.character(importer)) %>%
    
    left_join(country_codes %>% 
                mutate(country_code = as.character(country_code)) %>%  
                select(country_code, country_name) %>% 
                rename(exporter_name = country_name), 
              by = c("exporter" = "country_code")) %>%
    mutate(exporter = exporter_name) %>%
    select(-exporter_name) %>%  
    left_join(country_codes %>% 
                mutate(country_code = as.character(country_code)) %>%  
                select(country_code, country_name) %>% 
                rename(importer_name = country_name), 
              by = c("importer" = "country_code")) %>%
    mutate(importer = importer_name) %>%
    select(-importer_name)  

  return(data)
}

combined_data <- replace_country_codes(combined_data)


replace_product_codes <- function(data) {
  data <- data %>%
    mutate(product = as.character(product)) %>%
    
    left_join(product_codes %>% 
                mutate(code = as.character(code)) %>%  
                select(code, description),  
              by = c("product" = "code")) %>%
    
    mutate(product = description) %>%
    select(-description)  
    
  return(data)
}

combined_data <- replace_product_codes(combined_data)

remove_description_from_product <- function(data) {
  data <- data %>%
    mutate(product = sub(":.*", "", product))  

  return(data)
}

combined_data <- remove_description_from_product(combined_data)

merge_petroleum_rows <- function(data) {
  petroleum_data <- data %>%
    filter(grepl("petroleum", product, ignore.case = TRUE)) %>%
    group_by(year, exporter, importer) %>%
    summarise(
      value = sum(value, na.rm = TRUE),
      quantity = sum(quantity, na.rm = TRUE),
      product = "Petroleum Products",  
      .groups = "drop"
    )
  
  # Now remove the original petroleum rows and add the merged row
  data <- data %>%
    filter(!grepl("petroleum", product, ignore.case = TRUE)) %>%
    bind_rows(petroleum_data)
  
  return(data)
}

combined_data <- merge_petroleum_rows(combined_data)

merge_rows_by_group <- function(data) {
  data <- data %>%
    group_by(year, exporter, importer, product) %>%
    
    summarise(
      value = sum(value, na.rm = TRUE),
      quantity = sum(quantity, na.rm = TRUE),
      .groups = "drop"
    )
    
  return(data)
}

combined_data <- merge_rows_by_group(combined_data)
### 4958186 obs

uk_exporter_data <- combined_data %>%
  filter(exporter == "United Kingdom")
### 3385077 obs

uk_importer_data <- combined_data %>%
  filter(importer == "United Kingdom")
### 1588878 obs

importsum = sum(uk_importer_data$value)
exportsum = sum(uk_exporter_data$value)

# Clean up the environment, remove unnecessary variables
rm(col_spec, files, country_codes, product_codes, rename_columns, replace_country_codes, remove_description_from_product, merge_petroleum_rows, merge_rows_by_group)
gc()


```
Before analysis, the raw CEPII BACI dataset of more than 258 million records of international trade was pre-filtered to keep only rows where the UK imported or exported. This reduced the dataset to 13.4 million observations.

Product categories were originally 6-digit Harmonized System (HS) codes and country codes were numeric and both replaced with descriptive labels. Products were too broad in their detail (e.g., 21 varieties of meat, multiple varieties of coffee and eggs), so these were merged into more general product categories where feasible.

After cleansing and consolidation post, the dataset came down to 4.96 million observations around equal to `r (4973955 / 258605562 * 100)`% of the initial dataset. They consist of 3.39 million exports and 1.59 million imports.

In spite of additional export deals of quantity, this does not include greater trading value. There are greater total values of import over the period for UK compared to exports, with its net trading balance being `r exportsum - importsum`, which is equal to continuous deficit.

# Visualisation

## Trade value over time

Figure X illustrates the UK's net trade, import, and export totals between 1995 and 2022. Exports (purple) and imports (teal) exhibit steady growth up to 2008, when they varied depending on world economic turning points. Net trade (yellow), always in negative figures, shows a persistent and growing trade deficit—especially after 2015.

Severe economic shocks such as the 2008 financial crisis and pandemic 2020 triggered sharp declines in trade, then partial recoveries. Post-Brexit (2016) onwards, export growth plateaus while imports remain unchanged, which indicates emerging new trade frictions.

This historical trade deficit bears witness to UK dependence on imports and highlights the challenge of export competitiveness.
``` {r}
combined_data %>%
  filter(exporter == "United Kingdom" | importer == "United Kingdom") %>%
  group_by(year) %>%
  summarise(
    export_value = sum(value[exporter == "United Kingdom"], na.rm = TRUE),
    import_value = sum(value[importer == "United Kingdom"], na.rm = TRUE),
    net_value = export_value - import_value,
    .groups = "drop"  
  ) %>%
  gather(key = "type", value = "value", export_value, import_value, net_value) %>%
  ggplot(aes(x = year, y = value, color = type)) +
  geom_line(size = 1) +
  scale_color_viridis(discrete = TRUE) +  
  labs(title = "Export, Import, and Net Value Over Time", x = "Year", y = "Value") +
  theme_minimal() +
  theme(legend.title = element_blank())  
```
## Stacked Area Plots

The stacked area plots indicate the UK's export and import values with its top ten trading partners between 1995 and 2023. Export values rose steadily until the late 2010s, led consistently by the US, Germany, France, and the Netherlands. Imports also rose, with Germany, China, and the US being leading suppliers. Notably, China's role expanded exponentially in the 2000s. Both graphs identify the UK's historic reliance on its key European and world partners, and the persistent import and export value deficit is a signal of a long-term structural trade deficit.

``` {r}
combined_data %>%
  filter(exporter == "United Kingdom") %>%
  group_by(year, importer) %>%
  summarise(total_export_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  group_by(year) %>%
  mutate(rank = dense_rank(desc(total_export_value))) %>%
  filter(rank <= 10) %>%  
  ungroup() %>%  
  complete(year, importer, fill = list(total_export_value = 0)) %>%  
  ggplot(aes(x = year, y = total_export_value, fill = importer)) +
  geom_area(alpha = 0.6) +  
  scale_fill_viridis(discrete = TRUE) +  
  labs(title = "Export Value Over Time by Top 10 Importers", x = "Year", y = "Export Value") +
  theme_minimal()
```

```{r}
combined_data %>%
  filter(importer == "United Kingdom") %>%
  group_by(year, exporter) %>%
  summarise(total_import_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  group_by(year) %>%
  mutate(rank = dense_rank(desc(total_import_value))) %>%
  filter(rank <= 10) %>%  
  ungroup() %>%  
  complete(year, exporter, fill = list(total_import_value = 0)) %>%
  ggplot(aes(x = year, y = total_import_value, fill = exporter)) +
  geom_area(alpha = 0.6) +  # Stacked area chart
  scale_fill_viridis(discrete = TRUE) +  
  labs(title = "Import Value Over Time by Top 10 Exporters", x = "Year", y = "Import Value") +
  theme_minimal()
```
# correlations

## Correlation between trade partners

These two correlation matrices highlight the UK's prime trading partners segregated according to whether they are being exported to (left) or imported from (right). Exporting, the UK is extremely positively correlated with most of Europe's partners—Germany, France, Ireland, and the Netherlands—sitting in synchronised export trends into these countries. The USA, being an important partner, is less correlated, likely reflecting differences in economic cycles. On the import side, the relationships are closer still, especially between EU countries, and could reflect coordinated supply chains and integrated buying from the single market. On aggregate, the matrices reveal strong regional trade linkages, useful for identifying co-movements in trade volume that impact future projections.
```{r}
top_10_importers_aggregated <- combined_data %>%
  filter(exporter == "United Kingdom") %>%
  group_by(importer) %>%
  summarise(total_import_value = sum(value, na.rm = TRUE)) %>%
  arrange(desc(total_import_value)) %>%
  top_n(10, total_import_value)

top_10_importer_data_agg <- combined_data %>%
  filter(exporter == "United Kingdom" & importer %in% top_10_importers_aggregated$importer) %>%
  group_by(year, importer) %>%
  summarise(total_value = sum(value, na.rm = TRUE)) %>%
  spread(key = importer, value = total_value)

importer_correlation_matrix <- cor(top_10_importer_data_agg[, -1], use = "complete.obs")

corrplot(importer_correlation_matrix, method = "circle", type = "upper", tl.cex = 0.8)
```

```{r}
top_10_exporters_aggregated <- combined_data %>%
  filter(importer == "United Kingdom") %>%
  group_by(exporter) %>%
  summarise(total_export_value = sum(value, na.rm = TRUE)) %>%
  arrange(desc(total_export_value)) %>%
  top_n(10, total_export_value)

top_10_exporter_data_agg <- combined_data %>%
  filter(importer == "United Kingdom" & exporter %in% top_10_exporters_aggregated$exporter) %>%
  group_by(year, exporter) %>%
  summarise(total_value = sum(value, na.rm = TRUE)) %>%
  spread(key = exporter, value = total_value)

exporter_correlation_matrix <- cor(top_10_exporter_data_agg[, -1], use = "complete.obs")

corrplot(exporter_correlation_matrix, method = "circle", type = "upper", tl.cex = 0.8)
```
```{r include=FALSE}
##removing all unused data from the enviroment to save on ram

rm(list = setdiff(ls(), c("combined_data")))
gc()
```

# clustering
```{r}
# UK as Importer
uk_import_data <- combined_data %>%
  filter(importer == "United Kingdom")

# UK as Exporter
uk_export_data <- combined_data %>%
  filter(exporter == "United Kingdom")

# Top 20 exporters to UK
top_exporters <- uk_import_data %>%
  group_by(exporter) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  top_n(20, total_value) %>%
  pull(exporter)

# Top 20 import products
top_import_products <- uk_import_data %>%
  group_by(product) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  top_n(20, total_value) %>%
  pull(product)

# Top 20 importers from UK
top_importers <- uk_export_data %>%
  group_by(importer) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  top_n(20, total_value) %>%
  pull(importer)

# Top 20 export products
top_export_products <- uk_export_data %>%
  group_by(product) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  top_n(20, total_value) %>%
  pull(product)

exporter_matrix <- uk_import_data %>%
  filter(exporter %in% top_exporters) %>%
  group_by(exporter, year) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = year, values_from = total_value, values_fill = 0)

exporter_data <- exporter_matrix %>% select(-exporter)

set.seed(42)
exporter_clusters <- kmeans(exporter_data, centers = 4)

exporter_cluster_result <- exporter_matrix %>%
  mutate(cluster = exporter_clusters$cluster)

import_product_matrix <- uk_import_data %>%
  filter(product %in% top_import_products) %>%
  group_by(product, year) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = year, values_from = total_value, values_fill = 0)

import_product_data <- import_product_matrix %>% select(-product)

set.seed(42)
import_product_clusters <- kmeans(import_product_data, centers = 4)

import_product_cluster_result <- import_product_matrix %>%
  mutate(cluster = import_product_clusters$cluster)

importer_matrix <- uk_export_data %>%
  filter(importer %in% top_importers) %>%
  group_by(importer, year) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = year, values_from = total_value, values_fill = 0)

importer_data <- importer_matrix %>% select(-importer)

set.seed(42)
importer_clusters <- kmeans(importer_data, centers = 4)

importer_cluster_result <- importer_matrix %>%
  mutate(cluster = importer_clusters$cluster)

export_product_matrix <- uk_export_data %>%
  filter(product %in% top_export_products) %>%
  group_by(product, year) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = year, values_from = total_value, values_fill = 0)

export_product_data <- export_product_matrix %>% select(-product)

set.seed(42)
export_product_clusters <- kmeans(export_product_data, centers = 4)

export_product_cluster_result <- export_product_matrix %>%
  mutate(cluster = export_product_clusters$cluster)


##Visulasation
# Visualization for UK as Importer - Product Cluster
product_names_import <- import_product_cluster_result$product
product_values_import <- import_product_cluster_result %>%
  select(-product, -cluster) %>%
  as.matrix()
rownames(product_values_import) <- product_names_import

cor_matrix_import <- cor(t(product_values_import), use = "pairwise.complete.obs")
cor_edges_import <- which(upper.tri(cor_matrix_import) & cor_matrix_import > 0.8, arr.ind = TRUE)
edge_list_import <- data.frame(
  from = rownames(cor_matrix_import)[cor_edges_import[, 1]],
  to = rownames(cor_matrix_import)[cor_edges_import[, 2]]
)

node_info_import <- import_product_cluster_result %>%
  mutate(name = product,
         total_value = rowSums(select(., -product, -cluster), na.rm = TRUE)) %>%
  select(name, cluster, total_value)

graph_import <- graph_from_data_frame(d = edge_list_import, vertices = node_info_import, directed = FALSE)

# Visualization for UK as Exporter - Product Cluster
product_names_export <- export_product_cluster_result$product
product_values_export <- export_product_cluster_result %>%
  select(-product, -cluster) %>%
  as.matrix()
rownames(product_values_export) <- product_names_export

cor_matrix_export <- cor(t(product_values_export), use = "pairwise.complete.obs")
cor_edges_export <- which(upper.tri(cor_matrix_export) & cor_matrix_export > 0.8, arr.ind = TRUE)
edge_list_export <- data.frame(
  from = rownames(cor_matrix_export)[cor_edges_export[, 1]],
  to = rownames(cor_matrix_export)[cor_edges_export[, 2]]
)

node_info_export <- export_product_cluster_result %>%
  mutate(name = product,
         total_value = rowSums(select(., -product, -cluster), na.rm = TRUE)) %>%
  select(name, cluster, total_value)

graph_export <- graph_from_data_frame(d = edge_list_export, vertices = node_info_export, directed = FALSE)

# Visualization for UK as Importer - Exporter Cluster
exporter_names_import <- exporter_cluster_result$exporter
exporter_values_import <- exporter_cluster_result %>%
  select(-exporter, -cluster) %>%
  as.matrix()
rownames(exporter_values_import) <- exporter_names_import

cor_matrix_exporter_import <- cor(t(exporter_values_import), use = "pairwise.complete.obs")
cor_edges_exporter_import <- which(upper.tri(cor_matrix_exporter_import) & cor_matrix_exporter_import > 0.8, arr.ind = TRUE)
edge_list_exporter_import <- data.frame(
  from = rownames(cor_matrix_exporter_import)[cor_edges_exporter_import[, 1]],
  to = rownames(cor_matrix_exporter_import)[cor_edges_exporter_import[, 2]]
)

node_info_exporter_import <- exporter_cluster_result %>%
  mutate(name = exporter,
         total_value = rowSums(select(., -exporter, -cluster), na.rm = TRUE)) %>%
  select(name, cluster, total_value)

graph_exporter_import <- graph_from_data_frame(d = edge_list_exporter_import, vertices = node_info_exporter_import, directed = FALSE)

# Visualization for UK as Exporter - Importer Cluster
importer_names_export <- importer_cluster_result$importer
importer_values_export <- importer_cluster_result %>%
  select(-importer, -cluster) %>%
  as.matrix()
rownames(importer_values_export) <- importer_names_export

cor_matrix_importer_export <- cor(t(importer_values_export), use = "pairwise.complete.obs")
cor_edges_importer_export <- which(upper.tri(cor_matrix_importer_export) & cor_matrix_importer_export > 0.8, arr.ind = TRUE)
edge_list_importer_export <- data.frame(
  from = rownames(cor_matrix_importer_export)[cor_edges_importer_export[, 1]],
  to = rownames(cor_matrix_importer_export)[cor_edges_importer_export[, 2]]
)

node_info_importer_export <- importer_cluster_result %>%
  mutate(name = importer,
         total_value = rowSums(select(., -importer, -cluster), na.rm = TRUE)) %>%
  select(name, cluster, total_value)

graph_importer_export <- graph_from_data_frame(d = edge_list_importer_export, vertices = node_info_importer_export, directed = FALSE)

```

The clustering diagrams reveal how UK trade relationships group naturally based on trade similarity. In the import clusters (left), the majority of European exporters—including Germany, Italy, Spain, and Belgium—form a dense, interconnected network, suggesting similar trade patterns and co-movement in supply. The USA and Asian partners appear more isolated, indicating distinct dynamics. The export clusters (right) show a comparable trend, with Western nations such as France, Germany, and Canada forming cohesive groups, while emerging markets like the UAE, India, and Singapore form their own separate hub. These clusters highlight patterns not obvious through raw trade values alone, revealing hidden structure that supports the motivation for using deep learning models to detect and learn such implicit trade relationships. Identifying these latent clusters helps inform model design by emphasizing the importance of modeling interaction effects between countries and products—precisely the kind of complex structure deep learning is well-suited to capture.
```{r}
# Plot for UK as Importer - Exporter Cluster
ggraph(graph_exporter_import, layout = "fr") +
  geom_edge_link(alpha = 0.2) +
  geom_node_point(aes(size = total_value, color = factor(cluster))) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  theme_void() +
  labs(title = "Exporter Clusters - UK Imports", color = "Cluster", size = "Total Value")
```
```{r}
# Plot for UK as Exporter - Importer Cluster
ggraph(graph_importer_export, layout = "fr") +
  geom_edge_link(alpha = 0.2) +
  geom_node_point(aes(size = total_value, color = factor(cluster))) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  theme_void() +
  labs(title = "Importer Clusters - UK Exports", color = "Cluster", size = "Total Value")
```

```{r include=FALSE}
#clenaing enviroment 
rm(list = setdiff(ls(), c("combined_data")))
gc()
```

# deep learning

## Methods

Two deep learning models were created to forecast UK trade values, one for when the United Kingdom is an importer and one for when it is an exporter. These models were implemented using the Keras API in TensorFlow (version 2.10.0) under the R environment. To prepare the data for modeling, all trade transactions worth less than 5,000 were removed in an effort to reduce noise and prevent the likelihood of overfitting. Categorical columns such as trade partners and product types were embedded to integers and passed through embedding layers to learn their internal representation. Numerical features such as trade year and trade value were normalized to have zero mean and unit variance.

Both models were trained using 30 epochs and a batch size of 216, selected after trying different combinations of hyperparameters. This configuration provided consistent stable performance and avoided convergence issues associated with larger or smaller batch sizes. Mixed precision training (with the mixed_float16 policy) was also enabled to improve training efficiency without sacrificing numerical accuracy.

The loss function used was mean squared error (MSE), with mean absolute error (MAE) tracked as a secondary performance measure. A validation split of 20% was used in training, and shuffling was enabled to aid generalization. Model performance was monitored while training via training history plots. Both models displayed steady convergence and minimal divergence between training and validation error curves, indicating high generalization and no overfitting evidence

```{r}
policy <- tf$keras$mixed_precision$Policy("mixed_float16")
combined_data_dl <- combined_data %>%
  select(-quantity)

# Filter data for UK Exporter
uk_export_data <- combined_data_dl %>%
  filter(exporter == "United Kingdom") %>%
  mutate(
    importer = as.integer(factor(importer)),  # Integer encoding for importer
    product = as.integer(factor(product))     # Integer encoding for product
  ) %>%
  filter(!is.na(importer) & !is.na(product) & !is.na(value) & !is.na(year))

# Filter data for UK Importer
uk_import_data <- combined_data_dl %>%
  filter(importer == "United Kingdom") %>%
  mutate(
    exporter = as.integer(factor(exporter)),  # Integer encoding for exporter
    product = as.integer(factor(product))     # Integer encoding for product
  ) %>%
  filter(!is.na(exporter) & !is.na(importer) & !is.na(product) & !is.na(value) & !is.na(year))

exporter_levels <- levels(factor(combined_data$exporter))
importer_levels <- levels(factor(combined_data$importer))


product_levels <- levels(factor(combined_data$product))

# Normalize numerical columns (value, year)
uk_export_data <- uk_export_data %>%
  mutate(
    value = scale(value),
    year = scale(year)
  )

uk_import_data <- uk_import_data %>%
  mutate(
    value = scale(value),
    year = scale(year)
  )

# Prepare features (X) and target (y) for predictions
X_exporter <- uk_export_data %>% select(-value,-exporter)
y_exporter <- uk_export_data$value



X_importer <- uk_import_data %>% select(-value,-importer)
y_importer <- uk_import_data$value

# Model for UK as Exporter (with embeddings for categorical variables)
importer_input <- layer_input(shape = 1, name = "importer_input")
product_input <- layer_input(shape = 1, name = "product_input")

importer_embedding <- importer_input %>%
  layer_embedding(input_dim = length(unique(X_exporter$importer)) + 1, output_dim = 10)

product_embedding <- product_input %>%
  layer_embedding(input_dim = length(unique(X_exporter$product)) + 1, output_dim = 10)

importer_embedding_flat <- importer_embedding %>% layer_flatten()
product_embedding_flat <- product_embedding %>% layer_flatten()

numeric_input <- layer_input(shape = ncol(X_exporter) - 2, name = "numeric_input")  # Exclude 'importer' and 'product'

concatenated <- layer_concatenate(list(importer_embedding_flat, product_embedding_flat, numeric_input))

# Add Dense layers for processing
dense_layer <- concatenated %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, dtype = "float32")  # ✅ Ensures correct output type


exporter_model <- keras_model(
  inputs = list(importer_input, product_input, numeric_input),
  outputs = dense_layer
)

# Compile the model
exporter_model %>% compile(
  loss = "mse",  # Mean Squared Error loss function for regression
  optimizer = optimizer_adam(),
  metrics = c("mae")  # Mean Absolute Error for evaluation
)

# Prepare the data for training

importer_input_data <- as.integer(X_exporter$importer)
product_input_data <- as.integer(X_exporter$product)

numeric_input_data <- X_exporter %>%
  select(-importer, -product) %>%
  as.matrix()  # Ensure it’s numeric and in matrix form


y_exporter <- as.numeric(y_exporter)



```

# Training
## Exporter Model Training Performance
 
The exporter model shows a steady decrease in training loss and MAE, indicating consistent learning over the 30 epochs. While validation loss is decreasing early on as well, it begins to stabilize and fluctuate from roughly epoch 15, which means that while the model generalizes reasonably well, its capacity for continued improvement decreases. Validation MAE remains close to training MAE across the board with no clear sign of overfitting but with a hint of limited predictive variability acquired for unseen observations.

```{r include=FALSE}
# Train the UK Exporter model
exporter_history <- exporter_model %>% fit(
  x = list(
    importer_input = importer_input_data,
    product_input = product_input_data,
    numeric_input = numeric_input_data
  ),
  y = y_exporter,
  epochs = 30,
  batch_size = 216,
  validation_split = 0.2,
  shuffle = TRUE  # ✅ Add this
)

```
```{r}
plot(exporter_history) + ggtitle("Exporter Model Training History")
```

```{r include=FALSE}
# ----------------------------------------
# Model for UK as Importer (with embeddings for categorical variables)
# ----------------------------------------

# Model for UK as Importer (similar to Exporter)
importer_input <- layer_input(shape = 1, name = "importer_input")
product_input <- layer_input(shape = 1, name = "product_input")

importer_embedding <- importer_input %>%
  layer_embedding(input_dim = length(unique(X_importer$exporter)) + 1, output_dim = 10)

product_embedding <- product_input %>%
  layer_embedding(input_dim = length(unique(X_importer$product)) + 1, output_dim = 10)

importer_embedding_flat <- importer_embedding %>% layer_flatten()
product_embedding_flat <- product_embedding %>% layer_flatten()

numeric_input <- layer_input(shape = ncol(X_importer) - 2, name = "numeric_input")  # Exclude 'exporter' and 'product'

concatenated <- layer_concatenate(list(importer_embedding_flat, product_embedding_flat, numeric_input))

# Add Dense layers for processing
dense_layer <- concatenated %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, dtype = "float32")  # ✅ Ensures correct output type


importer_model <- keras_model(
  inputs = list(importer_input, product_input, numeric_input),
  outputs = dense_layer
)

# Compile the model
importer_model %>% compile(
  loss = "mse",  # Mean Squared Error loss function for regression
  optimizer = optimizer_adam(),
  metrics = c("mae")  # Mean Absolute Error for evaluation
)

# Prepare the data for training
importer_input_data <- as.integer(X_importer$exporter)  # Convert 'exporter' to integer
product_input_data <- as.integer(X_importer$product)  # Convert 'product' to integer
numeric_input_data_importer <- X_importer %>%
  select(-exporter, -product) %>%  # Remove 'exporter', 'product', and 'value'
  as.matrix()  # Convert to matrix
```

## Importer Model Training Performance

The importer model also displays a comparable trend of shrinking training loss and MAE, and flattening validation metrics at around midpoint training. The gap between training and validation curves is reasonably small, giving credence to the model's stability. Validation MAE's slight decline in later epochs also indicates soft late-stage generalization improvement, but the occasional jitter indicates some batch composition sensitivity or minimal underfitting in more complex import relations.

```{r include=FALSE}
# Train the UK Importer model
importer_history <- importer_model %>% fit(
  x = list(
    importer_input = importer_input_data,
    product_input = product_input_data,
    numeric_input = numeric_input_data_importer
  ),
  y = y_importer,
  epochs = 30,
  batch_size = 216,
  validation_split = 0.2,
  shuffle = TRUE  # ✅ Add this
)
```

```{r}
plot(importer_history) + ggtitle("Importer Model Training History")
```

```{r}
# ----------------------------------------
# Make Predictions for UK Exporter Model
# ----------------------------------------
# Determine the range of future years

# ----------------------------------------
# Make Predictions for UK Exporter Model
# ----------------------------------------
# Determine the range of future years

future_years <- 2020:2030

# 1. Get the top 10 products and countries (by total trade value)
top10_export_products <- uk_export_data %>%
  group_by(product) %>%
  summarise(total_value = sum(value)) %>%
  arrange(desc(total_value)) %>%
  slice_head(n = 10) %>%
  pull(product)

top10_import_products <- uk_import_data %>%
  group_by(product) %>%
  summarise(total_value = sum(value)) %>%
  arrange(desc(total_value)) %>%
  slice_head(n = 10) %>%
  pull(product)

top10_export_countries <- uk_export_data %>%
  group_by(importer) %>%
  summarise(total_value = sum(value)) %>%
  arrange(desc(total_value)) %>%
  slice_head(n = 10) %>%
  pull(importer)

top10_import_countries <- uk_import_data %>%
  group_by(exporter) %>%
  summarise(total_value = sum(value)) %>%
  arrange(desc(total_value)) %>%
  slice_head(n = 10) %>%
  pull(exporter)

# Helper function to scale years
scale_year <- function(years, mean_val, sd_val) {
  return((years - mean_val) / sd_val)
}

# Store original year scaling stats
year_mean_export <- attr(uk_export_data$year, "scaled:center")
year_sd_export <- attr(uk_export_data$year, "scaled:scale")
year_mean_import <- attr(uk_import_data$year, "scaled:center")
year_sd_import <- attr(uk_import_data$year, "scaled:scale")

# Prepare export_forecast_data for predictions
export_forecast_data <- data.frame(
  year = rep(future_years, each = length(top10_export_products)),
  product = rep(top10_export_products, times = length(future_years)),
  importer = rep(top10_export_countries, times = length(future_years)),
  value = NA  # Placeholder for predicted values
)

# Make predictions
export_preds <- exporter_model %>% predict(list(
  importer_input = as.integer(export_forecast_data$importer),
  product_input = as.integer(export_forecast_data$product),
  numeric_input = as.matrix(scale_year(export_forecast_data$year, year_mean_export, year_sd_export))
))

export_forecast_data$predicted_value <- as.numeric(export_preds * 
  attr(uk_export_data$value, "scaled:scale") + 
  attr(uk_export_data$value, "scaled:center"))

# Prepare import_forecast_data for predictions
import_forecast_data <- data.frame(
  year = rep(future_years, each = length(top10_import_products)),
  product = rep(top10_import_products, times = length(future_years)),
  exporter = rep(top10_import_countries, times = length(future_years)),
  value = NA  # Placeholder for predicted values
)

# Make predictions
import_preds <- importer_model %>% predict(list(
  importer_input = as.integer(import_forecast_data$exporter),
  product_input = as.integer(import_forecast_data$product),
  numeric_input = as.matrix(scale_year(import_forecast_data$year, year_mean_import, year_sd_import))
))

import_forecast_data$predicted_value <- as.numeric(import_preds * 
  attr(uk_import_data$value, "scaled:scale") + 
  attr(uk_import_data$value, "scaled:center"))

# Decoding
export_forecast_data$importer_label <- importer_levels[export_forecast_data$importer]
export_forecast_data$product_label <- product_levels[export_forecast_data$product]


import_forecast_data$exporter_label <- exporter_levels[import_forecast_data$exporter]
import_forecast_data$product_label <- product_levels[import_forecast_data$product]




```
## Summary of training
Both exporter and importer models register consistent decreases in training loss and mean absolute error (MAE), with validation curves platuing halfway through training. The comparatively narrow and consistent gap between training and validation metrics is an expression of high generalization and minimum risk of overfitting. Whereas the exporter model is consistently performing well across, the importer model experiences moderate declines in late-stage performance and occasional fluctuations, presumably as a result of batch variation or more intricate import dynamics. In general, both models converged and are suitable for prediction.

#Results
## Forecasted Total Trade Overview
Figure X illustrates estimated overall UK imports, exports, and net trade from 2023 to 2030. Imports (red) and exports (blue) are both estimated to gradually decline, with imports always in excess, once again maintaining a chronic rising trade deficit (black dashed line). This may reflect increasing dependence on imported foreign goods and poorer export performance.

```{R}
### TOTAL




# Calculate the total exports and imports
export_totals <- export_forecast_data %>%
  group_by(year) %>%
  summarise(export_value = sum(predicted_value)) %>%
  ungroup()

import_totals <- import_forecast_data %>%
  group_by(year) %>%
  summarise(import_value = sum(predicted_value)) %>%
  ungroup()

# Combine the exports and imports into one data frame
combined_totals <- full_join(export_totals, import_totals, by = "year") %>%
  mutate(
    # Calculate net trade: export_value - import_value
    net_trade = import_value - export_value,
  )

# Plot the results
ggplot(combined_totals, aes(x = year)) +
  geom_line(aes(y = export_value, color = "Export"), size = 1) +
  geom_line(aes(y = import_value, color = "Import"), size = 1) +
  geom_line(aes(y = net_trade), color = "black", linetype = "dashed", size = 1) +
  labs(
    title = "Forecasted Total Import & Export (with Net Trade)",
    y = "Trade Value",
    color = "Trade Type"
  ) +
  scale_color_manual(values = c("Export" = "blue", "Import" = "red")) +
  theme_minimal()

```

# Export top 10 producs

Figure X displays export forecasts by commodity. Vehicles remain the biggest but are projected to plummet by a huge margin. Other huge groups such as medicaments, engines, and petroleum products also fall, though at a more moderate pace. The simultaneous tendencies across goods are explained by the autoregressive characteristics of the model, which has a tendency to yield conservative and smoothened forecasts.

```{R}

ggplot(export_forecast_data %>% filter(product %in% top10_export_products),
       aes(x = year, y = predicted_value, color = product_label)) +
  stat_summary(fun = sum, geom = "line") +
  labs(title = "Forecasted Export Value for Top 10 Products", y = "Export Value")
```

# Import top 10 producs

Figure X shows UK import projections of the country's leading imported goods. Oils and cars lead in terms of number, with most categories—like metals, chemicals, and machinery—exhibiting flat or gradually declining trends. As indicated above, the smoothing bias of the model may downwardly bias volatility in sectoral demand.

```{r}
import_forecast_data$short_label <- stringr::str_trunc(import_forecast_data$product_label, width = 30)

ggplot(import_forecast_data %>%
         filter(product %in% top10_import_products),
       aes(x = year, y = predicted_value, color = short_label)) +
  geom_line() +
  labs(title = "Forecasted Import Value for Top 10 Products", y = "Import Value", color = "Product") +
  theme_minimal()

```

# Export top 10 Countries

Figure X displays forecasted export values to the UK's main trading partners. The USA remains the biggest recipient, followed by Germany and the Netherlands. All partners, though, show marginal decreases in export value, with no sign of growth in any nation, in support of the forecasted export contraction.

```{r}
# importer is the receiving country for export data
ggplot(export_forecast_data %>% 
         filter(importer %in% top10_export_countries) %>%
         mutate(importer_label = importer_levels[importer]),
       aes(x = year, y = predicted_value, color = importer_label)) +
  stat_summary(fun = sum, geom = "line") +
  labs(title = "Forecasted Export Value by Top 10 Countries", y = "Export Value")
```

# Import top 10 countries
Figure X shows UK imports from its top suppliers. Germany and China dominate, with other countries such as the USA, France, and the Netherlands showing similar falling trends. That none of these exporters is expected to increase indicates a general downturn in incoming trade.

```{r}
# exporter is the sending country for import data
ggplot(import_forecast_data %>%
         filter(exporter %in% top10_import_countries) %>%
         mutate(exporter_label = exporter_levels[exporter]),
       aes(x = year, y = predicted_value, color = exporter_label)) +
  stat_summary(fun = sum, geom = "line") +
  labs(title = "Forecasted Import Value by Top 10 Countries", y = "Import Value")
```
## Deep learning conclusion

The deep learning models developed for UK trade value prediction operated in a consistent way for both export and import scenarios. By encoding categorical variables and normalizing numerical values, the models picked up on general trends within historical trade data and made consistent predictions across countries and products.

However, due to the autoregressive nature of the forecasting process—where each future value is forecasted in relation to previously forecasted inputs—the results are smoothed and conservative. This is evidenced in the flat or gradually declining trends across all groups. Although reflecting the robustness of the model in identifying patterns, this limits the responsiveness of the model to abrupt economic shocks, policy shifts, or random changes in demand.

Lastly, the models provide a helpful high-level view of potential future trade patterns, highlighting the UK's persistent trade deficit and worsening export performance. However, for short-term forecasting or sectoral detail, complementary methods employing external variables or real-time indicators may be required to capture the complete richness of international trade.

# Final conclusion

This report explored the potential for forecasting UK international trade flows using data-driven methods. The primary research questions—whether to expect expansion or a decline in UK trade, which partners and products will dominate future flows, and what are the pitfalls of forecasting long-term trade—have been addressed by a multi-stage investigation.

After data cleaning and visualization of CEPII BACI trade data, UK export and import trends were analyzed over time. Product and country level trends indicated underlying interdependencies and long-term dynamics, e.g., impacts of global events and shifting trade alliances. Cluster analysis identified hidden groupings among goods and trading partners, offering insight into interdependent economic activities.

The core forecasting task was met with two deep learning models, one for UK imports and one for UK exports. These models captured general directional tendencies and produced stable forecasts but also demonstrated hallmark weaknesses of autoregressive deep learning, such as smoothing of uncertainty and subdued short-run volatility. For each of these, the models appropriately forecast future decreases in trade volumes and expanding trade deficits—results of considerable economic and policy significance.

Taken together, the analysis demonstrates how machine learning can provide valuable insight into the future of UK trade, along with the need for careful interpretation of long-term projections from data-driven models.

# Reference

https://www.cepii.fr/CEPII/en/bdd_modele/bdd_modele_item.asp?id=37

# appendix
All code and supporting documents are available on github in the following perma link
https://github.com/maxaus2002/ASSESSMENT-2-for-MAST7220/tree/4ee2afca34a870aa6cfba57fd4bb3f3344b36c84

Full version of report available on github
Accompanying power point available on github (Mainly contains condesned versions of data ommitted from the condensed report but were present in the full report.)
