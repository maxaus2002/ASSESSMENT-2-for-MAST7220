---
title: "Condensed report"
author: "Max Austin - Mada2"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}

# Set global knitr chunk options to suppress console output in the final document
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

# Uncomment the following line to install TinyTeX (required for PDF rendering via LaTeX)
# install.packages("tinytex")
# tinytex::install_tinytex()

# Load essential libraries; install if not already present
# install.packages("readr")
library(readr)      # For reading CSV and tabular data

# install.packages("dplyr")
library(dplyr)      # For data manipulation using tidy verbs

# install.packages("tidyr")
library(tidyr)      # For reshaping and cleaning datasets

# install.packages("stringr")
library(stringr)    # For consistent string manipulation

# install.packages("ggplot2")
library(ggplot2)    # For building layered graphics

# install.packages("viridis")
library(viridis)    # For colorblind-friendly visual palettes

# install.packages("treemap")
library(treemap)    # For hierarchical and categorical data visualization

# install.packages("corrplot")
library(corrplot)   # For plotting correlation matrices

# install.packages("cluster")
library(cluster)    # For clustering algorithms such as k-means and PAM

# install.packages("factoextra")
library(factoextra) # For extracting and visualizing clustering results

# install.packages("knitr")
library(knitr)      # For dynamic report generation

# install.packages("igraph")
library(igraph)     # For constructing network graphs

# install.packages("ggraph")
library(ggraph)     # For graphing network data using ggplot2 grammar

# install.packages("tidygraph")
library(tidygraph)  # For manipulating graphs using tidy syntax

#install.packages("data.table")
library(data.table)

# install.packages("reticulate")
# install.packages("tensorflow")
# install.packages("keras")
library(reticulate) # For integrating Python (needed for TensorFlow)
# reticulate::virtualenv_remove("r-tensorflow") # Optional: remove env if troubleshooting

library(keras)      # High-level neural network API running on TensorFlow
# keras::install_keras(method = "virtualenv", envname = "r-tensorflow") # Setup environment
# reticulate::use_virtualenv("r-tensorflow", required = TRUE)

library(tensorflow) # Backend for Keras with GPU acceleration support
# reticulate::py_config() # Uncomment to verify Python + TensorFlow setup
# tf$constant("Hello TensorFlow!") # Test line for verifying tensorflow

```
# Introduction

This paper analyzes the United Kingdom's international trade activity from 1995 to 2023 using data from the CEPII BACI database. The aim is to test whether future UK trade values—total trade, product-level flows, and country-level trade patterns—can be accurately forecasted using current data-based techniques.

To answer this, the report focuses on three significant questions: Will UK volumes of trade rise or fall in coming years? What goods and trading partners will set future trends? And what is difficult when attempting to forecast long-term trade?

The original data comprises over 258 million observations of trade between 238 nations and 5,000 products. It is reduced to UK trade only as an importer or exporter. The work is conducted in four stages: initial exploration and visualization, correlation analysis, product and country clustering, and finally, deep learning models for prediction.

Additional exploratory plots and additional figures are provided in the supplementary PowerPoint appendix due to space constraints.

# Exploration and Visualisation
```{r include=FALSE}
# =============================================================================
# Data Import and Preprocessing
# =============================================================================

# Define column specifications for consistent parsing of CSV data
col_spec <- cols(
  k = col_character(),  # Product code
  t = col_double(),     # Year
  i = col_double(),     # Exporter country code
  j = col_double(),     # Importer country code
  v = col_double(),     # Trade value
  q = col_double()      # Quantity
)

# Load all annual trade data CSV files into a list
files <- list.files(pattern = "*.csv")
data_list <- lapply(files[1:29], function(file) {
  read_csv(file, col_types = col_spec)
})

# Load metadata for decoding numeric country and product codes
country_codes <- read_csv("codes/country_codes_V202501.csv")
product_codes <- read_csv("codes/product_codes_HS92_V202501.csv")  

# Rename columns for clarity and consistency across datasets
rename_columns <- function(data) {
  data <- data %>%
    rename(
      year = t,
      exporter = i,
      importer = j,
      product = k,
      value = v,
      quantity = q
    )
  return(data)
}
data_list <- lapply(data_list, rename_columns)

# Combine data from all years into a single dataset
combined_data <- bind_rows(data_list)
# Initial dataset size: 258,605,562 observations

# Free up memory by removing intermediate data objects
rm(data_list)
gc()

# Filter dataset to retain only records involving the UK as importer or exporter
combined_data <- combined_data %>%
  filter(exporter == "826" | importer == "826")
# Dataset size after UK filtering: 13,367,259 observations

# Replace numeric country codes with human-readable country names
replace_country_codes <- function(data) {
  data <- data %>%
    mutate(exporter = as.character(exporter),
           importer = as.character(importer)) %>%
    left_join(country_codes %>% 
                mutate(country_code = as.character(country_code)) %>%  
                select(country_code, country_name) %>% 
                rename(exporter_name = country_name), 
              by = c("exporter" = "country_code")) %>%
    mutate(exporter = exporter_name) %>%
    select(-exporter_name) %>%
    left_join(country_codes %>% 
                mutate(country_code = as.character(country_code)) %>%  
                select(country_code, country_name) %>% 
                rename(importer_name = country_name), 
              by = c("importer" = "country_code")) %>%
    mutate(importer = importer_name) %>%
    select(-importer_name)
  
  return(data)
}
combined_data <- replace_country_codes(combined_data)

# Replace product codes with human-readable product descriptions
replace_product_codes <- function(data) {
  data <- data %>%
    mutate(product = as.character(product)) %>%
    left_join(product_codes %>% 
                mutate(code = as.character(code)) %>%  
                select(code, description),  
              by = c("product" = "code")) %>%
    mutate(product = description) %>%
    select(-description)
  
  return(data)
}
combined_data <- replace_product_codes(combined_data)

# Simplify product labels by removing detailed text after colons
remove_description_from_product <- function(data) {
  data <- data %>%
    mutate(product = sub(":.*", "", product))
  
  return(data)
}
combined_data <- remove_description_from_product(combined_data)

# Merge various petroleum-related products into a single "Petroleum Products" category
merge_petroleum_rows <- function(data) {
  petroleum_data <- data %>%
    filter(grepl("petroleum", product, ignore.case = TRUE)) %>%
    group_by(year, exporter, importer) %>%
    summarise(
      value = sum(value, na.rm = TRUE),
      quantity = sum(quantity, na.rm = TRUE),
      product = "Petroleum Products",
      .groups = "drop"
    )
  
  data <- data %>%
    filter(!grepl("petroleum", product, ignore.case = TRUE)) %>%
    bind_rows(petroleum_data)
  
  return(data)
}
combined_data <- merge_petroleum_rows(combined_data)

# Aggregate duplicate rows by key identifiers (year, exporter, importer, product)
merge_rows_by_group <- function(data) {
  data <- data %>%
    group_by(year, exporter, importer, product) %>%
    summarise(
      value = sum(value, na.rm = TRUE),
      quantity = sum(quantity, na.rm = TRUE),
      .groups = "drop"
    )
  return(data)
}
combined_data <- merge_rows_by_group(combined_data)
# Final dataset size: 4,958,186 observations

# Create separate dataset where UK acts as exporter
uk_exporter_data <- combined_data %>%
  filter(exporter == "United Kingdom")
# UK export dataset size: 3,385,077 observations

# Create separate dataset where UK acts as importer
uk_importer_data <- combined_data %>%
  filter(importer == "United Kingdom")
# UK import dataset size: 1,588,878 observations

# Compute total import and export values for summary purposes
importsum <- sum(uk_importer_data$value)
exportsum <- sum(uk_exporter_data$value)

```




Before analysis, the raw CEPII BACI dataset of more than 258 million records of international trade was pre-filtered to keep only rows where the UK imported or exported. This reduced the dataset to 13.4 million observations.

Product categories were originally 6-digit Harmonized System (HS) codes and country codes were numeric and both replaced with descriptive labels. Products were too broad in their detail (e.g., 21 varieties of meat, multiple varieties of coffee and eggs), so these were merged into more general product categories where feasible.

After cleansing and consolidation post, the dataset came down to 4.96 million observations around equal to `r (4973955 / 258605562 * 100)`% of the initial dataset. They consist of 3.39 million exports and 1.59 million imports.

In spite of additional export deals of quantity, this does not include greater trading value. There are greater total values of import over the period for UK compared to exports, with its net trading balance being `r exportsum - importsum`, which is equal to continuous deficit.

```{r include=FALSE}
# Remove all objects from the environment except 'combined_data' to reduce memory usage
rm(list = setdiff(ls(), c("combined_data")))
gc()  # Run garbage collection to free up unused memory

```

# Visualisation

## Trade value over time

Figure X illustrates the UK's net trade, import, and export totals between 1995 and 2022. Exports (purple) and imports (teal) exhibit steady growth up to 2008, when they varied depending on world economic turning points. Net trade (yellow), always in negative figures, shows a persistent and growing trade deficit—especially after 2015.

Severe economic shocks such as the 2008 financial crisis and pandemic 2020 triggered sharp declines in trade, then partial recoveries. Post-Brexit (2016) onwards, export growth plateaus while imports remain unchanged, which indicates emerging new trade frictions.

This historical trade deficit bears witness to UK dependence on imports and highlights the challenge of export competitiveness.
``` {r}
# =============================================================================
# Visualization: Total Export, Import, and Net Trade Values for the UK Over Time
# =============================================================================

combined_data %>%
  # Retain records where the UK is involved as either exporter or importer
  filter(exporter == "United Kingdom" | importer == "United Kingdom") %>%

  # Calculate annual totals for exports, imports, and net trade balance
  group_by(year) %>%
  summarise(
    export_value = sum(value[exporter == "United Kingdom"], na.rm = TRUE),  # Annual UK exports
    import_value = sum(value[importer == "United Kingdom"], na.rm = TRUE),  # Annual UK imports
    net_value = export_value - import_value,                                # Annual net trade balance
    .groups = "drop"
  ) %>%

  # Convert dataset to long format suitable for plotting
  gather(key = "type", value = "value", export_value, import_value, net_value) %>%

  # Generate line plot illustrating trade values over time
  ggplot(aes(x = year, y = value, color = type)) +
  geom_line(size = 1) +
  scale_color_viridis(discrete = TRUE) +  # Apply colorblind-friendly palette
  labs(
    title = "Export, Import, and Net Value Over Time",
    x = "Year",
    y = "Value"
  ) +
  theme_minimal() +
  theme(legend.title = element_blank())  # Remove legend title for clarity


```
## Stacked Area Plots

The stacked area plots indicate the UK's export and import values with its top ten trading partners between 1995 and 2023. Export values rose steadily until the late 2010s, led consistently by the US, Germany, France, and the Netherlands. Imports also rose, with Germany, China, and the US being leading suppliers. Notably, China's role expanded exponentially in the 2000s. Both graphs identify the UK's historic reliance on its key European and world partners, and the persistent import and export value deficit is a signal of a long-term structural trade deficit.

``` {r}
# =============================================================================
# Visualization: UK Export Values to Top 10 Importing Countries Over Time
# =============================================================================

combined_data %>%
  # Retain records where the UK acts as exporter
  filter(exporter == "United Kingdom") %>%

  # Calculate total export values grouped by year and importer
  group_by(year, importer) %>%
  summarise(total_export_value = sum(value, na.rm = TRUE), .groups = "drop") %>%

  # Rank importers each year by total export value, retaining top 10 annually
  group_by(year) %>%
  mutate(rank = dense_rank(desc(total_export_value))) %>%
  filter(rank <= 10) %>%

  ungroup() %>%

  # Complete missing year-importer combinations to ensure continuous plotting
  complete(year, importer, fill = list(total_export_value = 0)) %>%

  # Generate a stacked area chart showing export values over time
  ggplot(aes(x = year, y = total_export_value, fill = importer)) +
  geom_area(alpha = 0.6) +  # Apply transparency to improve readability
  scale_fill_viridis(discrete = TRUE) +  # Apply colorblind-friendly palette
  labs(
    title = "Export Value Over Time by Top 10 Importers",
    x = "Year",
    y = "Export Value"
  ) +
  theme_minimal()

```

```{r}
# =============================================================================
# Visualization: UK Import Values from Top 10 Exporting Countries Over Time
# =============================================================================

combined_data %>%
  # Filter data to retain only records where the UK is the importer
  filter(importer == "United Kingdom") %>%
  
  # Calculate total import value grouped by year and exporting country
  group_by(year, exporter) %>%
  summarise(total_import_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  
  # Within each year, rank exporters by total import value
  group_by(year) %>%
  mutate(rank = dense_rank(desc(total_import_value))) %>%
  filter(rank <= 10) %>%  # Keep only the top 10 exporters per year
  
  ungroup() %>%
  
  # Fill in missing year-exporter combinations with zero for smooth plotting
  complete(year, exporter, fill = list(total_import_value = 0)) %>%
  
  # Generate a stacked area chart to visualize import trends
  ggplot(aes(x = year, y = total_import_value, fill = exporter)) +
  geom_area(alpha = 0.6) +  # Add transparency for better layering visibility
  scale_fill_viridis(discrete = TRUE) +  # Colorblind-friendly palette
  labs(
    title = "Import Value Over Time by Top 10 Exporters",
    x = "Year",
    y = "Import Value"
  ) +
  theme_minimal()

```
# correlations

## Correlation between trade partners

These two correlation matrices highlight the UK's prime trading partners segregated according to whether they are being exported to (left) or imported from (right). Exporting, the UK is extremely positively correlated with most of Europe's partners—Germany, France, Ireland, and the Netherlands—sitting in synchronised export trends into these countries. The USA, being an important partner, is less correlated, likely reflecting differences in economic cycles. On the import side, the relationships are closer still, especially between EU countries, and could reflect coordinated supply chains and integrated buying from the single market. On aggregate, the matrices reveal strong regional trade linkages, useful for identifying co-movements in trade volume that impact future projections.
```{r}
# =============================================================================
# Correlation Analysis: UK Exports to Top 10 Importing Countries
# =============================================================================

# Identify the top 10 importing countries from the UK based on total export value
top_10_importers_aggregated <- combined_data %>%
  filter(exporter == "United Kingdom") %>%
  group_by(importer) %>%
  summarise(total_import_value = sum(value, na.rm = TRUE)) %>%
  arrange(desc(total_import_value)) %>%
  top_n(10, total_import_value)

# Aggregate annual export values for the top 10 importing countries
top_10_importer_data_agg <- combined_data %>%
  filter(exporter == "United Kingdom" & importer %in% top_10_importers_aggregated$importer) %>%
  group_by(year, importer) %>%
  summarise(total_value = sum(value, na.rm = TRUE)) %>%
  spread(key = importer, value = total_value)

# Compute the correlation matrix across importers over time
importer_correlation_matrix <- cor(top_10_importer_data_agg[, -1], use = "complete.obs")

# Visualize the correlation matrix using a circle-style plot
corrplot(importer_correlation_matrix, method = "circle", type = "upper", tl.cex = 0.8)

```

```{r}
# =============================================================================
# Correlation Analysis: Annual Export Trends to Top 10 Importing Countries
# =============================================================================

# Extract the top 10 countries that import the highest total trade value from the United Kingdom
top_10_importers_aggregated <- combined_data %>%
  filter(exporter == "United Kingdom") %>%
  group_by(importer) %>%
  summarise(total_import_value = sum(value, na.rm = TRUE)) %>%
  arrange(desc(total_import_value)) %>%
  top_n(10, total_import_value)

# Aggregate annual trade values for the top 10 importers from the United Kingdom
top_10_importer_data_agg <- combined_data %>%
  filter(exporter == "United Kingdom" & importer %in% top_10_importers_aggregated$importer) %>%
  group_by(year, importer) %>%
  summarise(total_value = sum(value, na.rm = TRUE)) %>%
  spread(key = importer, value = total_value)  # Reshape data to wide format for correlation analysis

# Compute the correlation matrix between importing countries based on annual trade values
importer_correlation_matrix <- cor(top_10_importer_data_agg[, -1], use = "complete.obs")

# Generate a circular correlation plot (upper triangle) for visual comparison between top importers
corrplot(importer_correlation_matrix, method = "circle", type = "upper", tl.cex = 0.8)


```
# Clustering




```{r include=FALSE}

# =============================================================================
# 1) Prepare Subsets: UK as Exporter and Importer
# =============================================================================

uk_export_data <- combined_data %>%
  filter(exporter == "United Kingdom") %>%
  drop_na(importer, product, year, value)

uk_import_data <- combined_data %>%
  filter(importer == "United Kingdom") %>%
  drop_na(exporter, product, year, value)

# =============================================================================
# 2) Identify Top-20 Trade Partners and Products by Export/Import Value
# =============================================================================

top_exporters <- uk_import_data %>%
  count(exporter, wt = value, name = "total_value") %>%
  slice_max(total_value, n = 20) %>% 
  pull(exporter)

top_importers <- uk_export_data %>%
  count(importer, wt = value, name = "total_value") %>%
  slice_max(total_value, n = 20) %>%
  pull(importer)

top_export_products <- uk_export_data %>%
  count(product, wt = value, name = "total_value") %>%
  slice_max(total_value, n = 20) %>%
  pull(product)

top_import_products <- uk_import_data %>%
  count(product, wt = value, name = "total_value") %>%
  slice_max(total_value, n = 20) %>%
  pull(product)

# =============================================================================
# 3) Clustering Function: K-means on Partner-Year Trade Value Matrix
# =============================================================================

cluster_time_series <- function(df, key_col, keep_vals) {
  mat <- df %>%
    filter(.data[[key_col]] %in% keep_vals) %>%
    group_by(.data[[key_col]], year) %>%
    summarise(total_value = sum(value, na.rm=TRUE), .groups="drop") %>%
    pivot_wider(names_from = year, values_from = total_value, values_fill = 0)
  vals <- mat %>% select(-all_of(key_col)) %>% as.matrix() %>% scale()
  set.seed(42)
  km <- kmeans(vals, centers = 4)
  mat %>% mutate(cluster = km$cluster)
}

# =============================================================================
# 4a) Clustering & Plotting: Exporters to UK (UK as Importer)
# =============================================================================

exp_clust <- cluster_time_series(
  df         = uk_import_data,
  key_col    = "exporter",
  keep_vals  = top_exporters
)

# Build edge list based on high correlation between time series
exp_mat <- exp_clust %>% select(-exporter, -cluster) %>% as.matrix()
rownames(exp_mat) <- exp_clust$exporter
cor_exp <- cor(t(exp_mat), use = "pairwise.complete.obs")
edges_exp <- which(upper.tri(cor_exp) & cor_exp > 0.8, arr.ind = TRUE)
edge_df_exp <- data.frame(
  from = rownames(cor_exp)[edges_exp[,1]],
  to   = rownames(cor_exp)[edges_exp[,2]],
  weight = cor_exp[edges_exp]
)

# Prepare node attributes: cluster and trade volume
nodes_exp <- exp_clust %>%
  mutate(name = exporter, total_value = rowSums(select(., -exporter, -cluster))) %>%
  select(name, cluster, total_value)

# Create graph object and visualize using ggraph
graph_exporter_import <- graph_from_data_frame(edge_df_exp, vertices = nodes_exp, directed = FALSE)

ggraph(graph_exporter_import, layout = "fr") +
  geom_edge_link(aes(width = weight), alpha = 0.3) +
  geom_node_point(aes(size = total_value, color = factor(cluster))) +
  geom_node_text(aes(label = name), repel = TRUE, size = 2) +
  theme_void() +
  labs(
    title = "Clusters of Countries Exporting to the UK",
    color = "Cluster", size = "Total Value"
  )

# =============================================================================
# 4b) Clustering & Plotting: Importers from UK (UK as Exporter)
# =============================================================================

imp_clust <- cluster_time_series(
  df         = uk_export_data,
  key_col    = "importer",
  keep_vals  = top_importers
)

imp_mat <- imp_clust %>% select(-importer, -cluster) %>% as.matrix()
rownames(imp_mat) <- imp_clust$importer
cor_imp <- cor(t(imp_mat), use = "pairwise.complete.obs")
edges_imp <- which(upper.tri(cor_imp) & cor_imp > 0.8, arr.ind = TRUE)
edge_df_imp <- data.frame(
  from = rownames(cor_imp)[edges_imp[,1]],
  to   = rownames(cor_imp)[edges_imp[,2]],
  weight = cor_imp[edges_imp]
)

nodes_imp <- imp_clust %>%
  mutate(name = importer, total_value = rowSums(select(., -importer, -cluster))) %>%
  select(name, cluster, total_value)

graph_importer_export <- graph_from_data_frame(edge_df_imp, vertices = nodes_imp, directed = FALSE)

ggraph(graph_importer_export, layout = "fr") +
  geom_edge_link(aes(width = weight), alpha = 0.3) +
  geom_node_point(aes(size = total_value, color = factor(cluster))) +
  geom_node_text(aes(label = name), repel = TRUE, size = 2) +
  theme_void() +
  labs(
    title = "Clusters of UK’s Export Destinations",
    color = "Cluster", size = "Total Value"
  )


```

The cluster plots reveal natural clusters of the UK's trading partners based on similarity in trade. European countries like Germany, France, and Spain are tightly networked groups within UK export clusters (importers), whereas partners like the USA and Ireland are isolated, which indicates different trade patterns.

The exporter clusters (UK imports) also show analogous tendencies, a core European group isolated from the likes of the USA, Canada, and China. The inherent structures demonstrate trade cohesion at a regional level and vindicate the use of deep learning models that can pick out such complex, not-so-clear patterns in trading behavior.


```{r}
# =============================================================================
# Visualization: UK Export Clusters — Graph of Top Importers
# =============================================================================

ggraph(graph_importer_export, layout = "fr") +
  geom_edge_link(alpha = 0.2) +  # Draw edges between correlated importers
  geom_node_point(aes(size = total_value, color = factor(cluster))) +  # Nodes sized by trade value, colored by cluster
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +  # Add node labels with repulsion to avoid overlap
  theme_void() +  # Clean background for better focus on network structure
  labs(
    title = "Importer Clusters - UK Exports",  # Plot title
    color = "Cluster",                         # Legend title for cluster color
    size = "Total Value"                       # Legend title for node size
  )

```

```{r}
# =============================================================================
# Visualization: UK Import Clusters — Graph of Top Exporting Countries
# =============================================================================

ggraph(graph_exporter_import, layout = "fr") +
  geom_edge_link(alpha = 0.2) +  # Draw edges between highly correlated exporters
  geom_node_point(aes(size = total_value, color = factor(cluster))) +  # Nodes sized by trade value and colored by cluster
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +  # Add country names as node labels with repulsion
  theme_void() +  # Remove axes and background elements for a cleaner network view
  labs(
    title = "Exporter Clusters - UK Imports",  # Plot title
    color = "Cluster",                         # Legend label for cluster groups
    size = "Total Value"                       # Legend label for total trade value
  )

```

```{r include=FALSE}
# Remove all objects from the environment except 'combined_data' to reduce memory usage
rm(list = setdiff(ls(), c("combined_data")))
gc()  # Run garbage collection to free up unused memory

```

# deep learning

## Methods
Two separate deep learning models were developed to forecast the United Kingdom's trade values—one for exports (UK as exporter) and one for imports (UK as importer). Both models were run using the Keras API in R, with TensorFlow backing, and particularly used Long Short-Term Memory (LSTM) layers to capture the temporal dynamics inherent in the trade data most effectively. Prior to modeling, trades with trade values less than 2500 were removed to simplify and avoid overfitting. Trade values were then log-transformed using log1p to address skewness and stabilize variance. Three lag features (lag_1, lag_2, lag_3) were also derived based on previous trade values summarized by trading partner (importer/exporter), product, and year, which are specifically tailored to identify sequential trade patterns.

Categorical features, i.e., trading partners (exporter/importer countries) and product types, were factor-encoded as integer representations. The corresponding factor levels were stored separately to enable proper decoding after prediction. These integer-encoded features were converted to dense embedding vectors through learnable embedding layers so that the model is capable of learning useful representations of countries and products. Numeric variables like the year of trade and lagged trade attributes were standardized (zero mean and unit variance) for improved model performance and convergence.

For each model, the categorical embeddings (for products and trading partners) were concatenated, projected via dense layers, and repeated over time to be correctly aligned with the numeric time-series features. This tensor was then input into an LSTM layer that is able to learn complex sequential patterns. The output of the LSTM was passed through additional dense layers to generate a continuous prediction of trade value.

The importer model and exporter model were both individually trained for 20 epochs at a batch size of 216 and mixed-precision training (mixed_float16) for optimization of computational resources. Mean Squared Error (MSE) was the principal loss function with Mean Absolute Error (MAE) additionally tracked as an assessment metric. A 20% validation split and epoch-wise data shuffling were employed during training, with training curves monitored closely to maintain convergent stability. Future predictions for the 2020–2030 time period were then created by producing prediction grids of all potential combinations of partners and products. Predictions were then re-decoded back into their native units using retained scaling attributes and factor-level vectors to enable correct interpretation.

In contrast with conventional feedforward networks, such an LSTM-grounded architecture introduces a strong sequence-aware methodology in forecasting the UK's future trade values accurately.

```{r}
# SECTION 1: MIXED PRECISION
# Enable mixed precision (optional, if supported hardware)
policy <- tf$keras$mixed_precision$Policy("mixed_float16")
tf$keras$mixed_precision$set_global_policy(policy)

# =============================================================================
# 2) DATA PREPARATION
# =============================================================================
# combined_data has: year, exporter, importer, product, value, quantity
combined_data_dl <- combined_data %>%
  # drop quantity (unused)
  select(-quantity) %>%
  # denoise: remove very small trades
  filter(value >= 2500) %>%
  # log‐transform the value
  mutate(value = log1p(value))

# create lag_1, lag_2, lag_3 by exporter/importer/product groups
num_lags <- 3
generate_lags <- function(df, by_vars, value_var, n_lags) {
  for (lag in seq_len(n_lags)) {
    lag_name <- paste0("lag_", lag)
    df <- df %>%
      group_by(across(all_of(by_vars))) %>%
      arrange(year) %>%
      mutate(!!lag_name := dplyr::lag(.data[[value_var]], lag)) %>%
      ungroup()
  }
  df
}

combined_data_dl <- combined_data_dl %>%
  generate_lags(
    by_vars    = c("exporter","importer","product"),
    value_var  = "value",
    n_lags     = num_lags
  ) %>%
  # drop any rows missing lag_1/2/3
  filter(if_all(starts_with("lag_"), ~ !is.na(.)))
```

```{r}
# =============================================================================
# 3) SPLIT UK EXPORT & IMPORT DATASETS
# =============================================================================
# UK as EXPORTER → partner is importer
uk_export_data <- combined_data_dl %>%
  filter(exporter == "United Kingdom") %>%
  drop_na(importer, product) %>%
  mutate(
    importer = factor(importer),  # keep as factor
    product  = factor(product)
  )

# store these for decoding
exp_imp_levels  <- levels(uk_export_data$importer)
exp_prod_levels <- levels(uk_export_data$product)

# now convert to integer codes
uk_export_data <- uk_export_data %>%
  mutate(
    importer = as.integer(importer),
    product  = as.integer(product)
  )

# UK as IMPORTER → partner is exporter
uk_import_data <- combined_data_dl %>%
  filter(importer == "United Kingdom") %>%
  drop_na(exporter, product) %>%
  mutate(
    exporter = factor(exporter),
    product  = factor(product)
  )

imp_exp_levels  <- levels(uk_import_data$exporter)
imp_prod_levels <- levels(uk_import_data$product)

uk_import_data <- uk_import_data %>%
  mutate(
    exporter = as.integer(exporter),
    product  = as.integer(product)
  )

```

```{r}
# =============================================================================
# 4) NORMALIZE NUMERIC COLUMNS
# =============================================================================
scale_all <- function(df, cols) {
  for (c in cols) df[[c]] <- scale(df[[c]])
  df
}

scale_cols <- c("value","year", paste0("lag_",1:num_lags))
uk_export_data <- scale_all(uk_export_data, scale_cols)
uk_import_data <- scale_all(uk_import_data, scale_cols)


```

```{r include=FALSE}


# =============================================================================
# 5) DEFINE & COMPILE EXPORTER LSTM MODEL
# =============================================================================
# Inputs for exporter_model: importer_id, product_id, [year + lags] sequence
X_exp   <- uk_export_data %>% select(-value,-exporter)
y_exp   <- as.numeric(uk_export_data$value)

imp_in   <- layer_input(shape=1, name="importer_input")
prod_in  <- layer_input(shape=1, name="product_input")
num_in   <- layer_input(shape=c(ncol(X_exp)-2,1), name="numeric_input")

imp_emb  <- imp_in  %>% layer_embedding(input_dim=max(X_exp$importer)+1, output_dim=10) %>% layer_flatten()
prod_emb <- prod_in %>% layer_embedding(input_dim=max(X_exp$product)+1,  output_dim=10) %>% layer_flatten()

# project & repeat embeddings
emb_rep  <- layer_concatenate(list(imp_emb, prod_emb)) %>%
             layer_dense(20, activation="relu") %>%
             layer_repeat_vector(ncol(X_exp)-2)

merged   <- layer_concatenate(list(emb_rep, num_in), axis=-1) %>%
              layer_lstm(32) %>%
              layer_dense(16, activation="relu") %>%
              layer_dense(1, dtype="float32")

exporter_model <- keras_model(list(imp_in,prod_in,num_in), merged)
exporter_model %>% compile(
  loss      = "mse",
  optimizer = optimizer_adam(),
  metrics   = "mae"
)


```


```{r include=FALSE}

# =============================================================================
# 6) TRAIN EXPORTER MODEL
# =============================================================================
numeric_array_exp <- X_exp %>%
  select(year, starts_with("lag_")) %>%
  as.matrix() %>%
  array(dim=c(nrow(.), ncol(.), 1))

exporter_history <- exporter_model %>% fit(
  x = list(
    importer_input = as.integer(X_exp$importer),
    product_input  = as.integer(X_exp$product),
    numeric_input  = numeric_array_exp
  ),
  y               = y_exp,
  epochs          = 20,
  batch_size      = 216,
  validation_split= 0.2,
  shuffle         = TRUE
)

```
# Exporter moded training 

The exporter model trains fast, with both the training loss and MAE falling off sharply in the initial 5 epochs and flattening out soon after. The validation measures follow nearly the same trajectory, with no divergence between the validation and training curves. This indicates excellent generalization and no risk of overfitting. Both loss and MAE flatten early and remain flat from epoch 20, which indicates that the model reaches optimum performance early and maintains it for a long time.




```{r}
plot(exporter_history) + ggtitle("Exporter Model Training History")
```

```{r include=FALSE }
# =============================================================================
# 7) DEFINE & COMPILE IMPORTER LSTM MODEL
# =============================================================================
X_imp   <- uk_import_data %>% select(-value,-importer)
y_imp   <- as.numeric(uk_import_data$value)

imp2_in   <- layer_input(shape=1, name="exporter_input")
prod2_in  <- layer_input(shape=1, name="product_input")
num2_in   <- layer_input(shape=c(ncol(X_imp)-2,1), name="numeric_input")

exp_emb   <- imp2_in  %>% layer_embedding(input_dim=max(X_imp$exporter)+1, output_dim=10) %>% layer_flatten()
prod2_emb <- prod2_in %>% layer_embedding(input_dim=max(X_imp$product)+1,  output_dim=10) %>% layer_flatten()

emb2_rep  <- layer_concatenate(list(exp_emb, prod2_emb)) %>%
              layer_dense(20, activation="relu") %>%
              layer_repeat_vector(ncol(X_imp)-2)

merged2    <- layer_concatenate(list(emb2_rep, num2_in), axis=-1) %>%
               layer_lstm(32) %>%
               layer_dense(16, activation="relu") %>%
               layer_dense(1, dtype="float32")

importer_model <- keras_model(list(imp2_in,prod2_in,num2_in), merged2)
importer_model %>% compile(
  loss      = "mse",
  optimizer = optimizer_adam(),
  metrics   = "mae"
)



```


```{r include=FALSE}
# =============================================================================
# 8) TRAIN IMPORTER MODEL
# =============================================================================
numeric_array_imp <- X_imp %>%
  select(year, starts_with("lag_")) %>%
  as.matrix() %>%
  array(dim=c(nrow(.), ncol(.), 1))

importer_history <- importer_model %>% fit(
  x = list(
    exporter_input = as.integer(X_imp$exporter),
    product_input  = as.integer(X_imp$product),
    numeric_input  = numeric_array_imp
  ),
  y               = y_imp,
  epochs          = 20,
  batch_size      = 216,
  validation_split= 0.2,
  shuffle         = TRUE
)

```
# Importer Model Training

The importer pattern has a linear, smooth reduction in training loss and MAE over the 20 epochs. Validation measures are larger to start and fluctuate more widely, especially after epoch 10, but level off in general. The consistent difference between training and validation curves suggests slight underfitting or limited ability to identify complex patterns in some import relationships. However, the model is stable in performance and does not overfit, suggesting good—but conservatively so—generalization.


```{r}
plot(importer_history) + ggtitle("Importer Model Training History")
```

```{r}
# =============================================================================
# 9) FORECAST HELPER & EXECUTION
# =============================================================================
make_forecast_fast <- function(
  model, train_df, years,
  cat1_vals, cat2_vals,
  year_mean, year_sd,
  cat1_name = "importer_input",
  cat2_name = "product_input",
  batch_size = NULL,
  workers = 4,
  use_multiprocessing = TRUE
) {
  # 1) Construct the grid with CJ()
  dt <- data.table::CJ(year = years, cat1 = cat1_vals, cat2 = cat2_vals)
  dt[, paste0("lag_", 1:3) := 0L]      # zero‑fill lags
  dt[, year := (year - year_mean) / year_sd]  # scale years in place

  # 2) Build the numeric tensor
  m     <- as.matrix(dt[, .(year, lag_1, lag_2, lag_3)])
  num_arr <- array(m, dim = c(nrow(m), ncol(m), 1))  # <-- use nrow() here

  # 3) Assemble inputs
  inputs <- list()
  inputs[[cat1_name]]      <- dt$cat1
  inputs[[cat2_name]]      <- dt$cat2
  inputs[["numeric_input"]] <- num_arr

  # 4) Predict in one big batch
  if (is.null(batch_size)) batch_size <- nrow(dt)
  preds <- model %>% predict(
    inputs,
    batch_size          = batch_size,
    verbose             = 0,
    workers             = workers,
    use_multiprocessing = use_multiprocessing
  )

  # 5) Back‑transform
  dt[, predicted_value := as.numeric(
    preds * attr(train_df$value, "scaled:scale") +
    attr(train_df$value, "scaled:center")
  )]

  dt
}

# Forecast years
future_years <- 2020:2030

# (a) Exporter side (UK → partner/import combos)
export_forecast <- make_forecast_fast(
  model       = exporter_model,
  train_df    = uk_export_data,
  years       = future_years,
  cat1_vals   = sort(unique(X_exp$importer)),
  cat2_vals   = sort(unique(X_exp$product)),
  year_mean   = attr(uk_export_data$year,"scaled:center"),
  year_sd     = attr(uk_export_data$year,"scaled:scale"),
  cat1_name   = "importer_input",
  cat2_name   = "product_input"
)

# (b) Importer side (UK ← partner/export combos)
import_forecast <- make_forecast_fast(
  model       = importer_model,
  train_df    = uk_import_data,
  years       = future_years,
  cat1_vals   = sort(unique(X_imp$exporter)),
  cat2_vals   = sort(unique(X_imp$product)),
  year_mean   = attr(uk_import_data$year,"scaled:center"),
  year_sd     = attr(uk_import_data$year,"scaled:scale"),
  cat1_name   = "exporter_input",   # matches importer_model’s first input
  cat2_name   = "product_input"
)

export_forecast <- export_forecast %>%
  mutate(
    importer_name = exp_imp_levels[cat1],   # correct mapping
    product_name  = exp_prod_levels[cat2]
  )

# decode import forecast
import_forecast <- import_forecast %>%
  mutate(
    exporter_name = imp_exp_levels[cat1],   # correct mapping
    product_name  = imp_prod_levels[cat2]
  )

```
## Summary of training
Both exporter and importer models have steep declines in training loss and mean absolute error (MAE) during early training. The exporter model converges smoothly, with validation metrics tracking training closely, showing good generalization and no sign of overfitting. The importer model also converges well, though with a slightly wider and more oscillating validation gap, which likely is a sign of the richness of import relationships. While there are some fluctuations in the late period, both models are stable and suitable for precise trade forecasting.

# Results
## Forecasted Total Trade Overview
Figure X graphically forecasts UK imports, exports, and net trade balance from 2020 to 2030 using an autoregressive LSTM model. Both imports (red) and exports (blue) are stable in the long run with imports consistently higher than exports. This forms a persistent trade deficit (black dashed line). The autoregressive framework—employing lagged values of trade—signifies that past trends in trade are strong predictors of future direction, sustaining the established structural trade deficit in UK trade.
```{R}
# =============================================================================
# 1) Recover the Unscaled “Year” for Plotting & Grouping
# =============================================================================

export_forecast <- export_forecast %>%
  mutate(
    year = year * attr(uk_export_data$year, "scaled:scale") +
           attr(uk_export_data$year, "scaled:center")
  )

import_forecast <- import_forecast %>%
  mutate(
    year = year * attr(uk_import_data$year, "scaled:scale") +
           attr(uk_import_data$year, "scaled:center")
  )

# =============================================================================
# 2) Compute Annual Totals for Exports, Imports, and Net Trade
# =============================================================================

export_totals <- export_forecast %>%
  group_by(year) %>%
  summarise(export_value = sum(predicted_value), .groups = "drop")

import_totals <- import_forecast %>%
  group_by(year) %>%
  summarise(import_value = sum(predicted_value), .groups = "drop")

# Merge export and import forecasts; compute net trade balance
combined_totals <- full_join(export_totals, import_totals, by = "year") %>%
  mutate(
    net_trade = import_value - export_value
  )

# =============================================================================
# 3) Plot Forecasted Exports, Imports, and Net Trade (2020–2030)
# =============================================================================

ggplot(combined_totals, aes(x = year)) +
  geom_line(aes(y = export_value, color = "Exports"), size = 1) +
  geom_line(aes(y = import_value, color = "Imports"), size = 1) +
  geom_line(aes(y = net_trade, color = "Net Balance"),
            linetype = "dashed", size = 1) +
  scale_color_manual(
    values = c("Exports" = "blue", "Imports" = "red", "Net Balance" = "black")
  ) +
  labs(
    title = "UK Trade Forecast (2020–2030)",
    x     = "Year",
    y     = "Predicted Trade Value",
    color = ""
  ) +
  theme_minimal()

```

# Export top 10 producs

Figure X presents the forecasted export values for the UK's top 10 product categories for the period 2020–2030. All the major goods—metals, petroleum products, engines, and vehicles—are found to increase steadily over time. The increasing trends are smooth as a result of the autoregressive LSTM model, which extrapolates based on recent growth trends and smooths over short-run volatility.

```{R}
# =============================================================================
# A) Compute the TRUE Top‑10 Lists by Raw (Untransformed) Trade Value
# =============================================================================

# 1) Identify Top-10 Export Products and Destination Countries (by total value)
uk_raw_export <- combined_data %>% filter(exporter == "United Kingdom")

top10_export_product_names <- uk_raw_export %>%
  group_by(product) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  slice_max(total_value, n = 10) %>%
  pull(product)

top10_export_country_names <- uk_raw_export %>%
  group_by(importer) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  slice_max(total_value, n = 10) %>%
  pull(importer)

# 2) Identify Top-10 Import Products and Source Countries (by total value)
uk_raw_import <- combined_data %>% filter(importer == "United Kingdom")

top10_import_product_names <- uk_raw_import %>%
  group_by(product) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  slice_max(total_value, n = 10) %>%
  pull(product)

top10_import_country_names <- uk_raw_import %>%
  group_by(exporter) %>%
  summarise(total_value = sum(value, na.rm = TRUE), .groups = "drop") %>%
  slice_max(total_value, n = 10) %>%
  pull(exporter)

# =============================================================================
# B) Decode Forecast Outputs into Human‑Readable Names
# =============================================================================

# (Assumes you preserved factor level vectors during training data preparation)
# - exp_imp_levels, exp_prod_levels: for decoding exporter model output
# - imp_exp_levels, imp_prod_levels: for decoding importer model output

export_forecast <- export_forecast %>%
  mutate(
    year          = year * attr(uk_export_data$year, "scaled:scale") +
                    attr(uk_export_data$year, "scaled:center"),
    product_name  = exp_prod_levels[cat2],
    importer_name = exp_imp_levels[cat1]
  )

import_forecast <- import_forecast %>%
  mutate(
    year          = year * attr(uk_import_data$year, "scaled:scale") +
                    attr(uk_import_data$year, "scaled:center"),
    product_name  = imp_prod_levels[cat2],
    exporter_name = imp_exp_levels[cat1]
  )

# =============================================================================
# C) Plot 1: Forecasted UK Exports by Top‑10 Products
# =============================================================================

export_forecast %>%
  filter(product_name %in% top10_export_product_names) %>%
  ggplot(aes(x = year, y = predicted_value, color = product_name)) +
    stat_summary(fun = sum, geom = "line", size = 1) +
    labs(
      title = "Forecasted UK Exports by Top 10 Products",
      x     = "Year",
      y     = "Total Predicted Export Value",
      color = "Product"
    ) +
    theme_minimal()


```

# Import top 10 producs

Figure X shows estimated UK import values for the leading 10 product categories from 2020 through 2030. Most of the large categories—like vehicles, data processing equipment, and petroleum goods—are likely to rise steadily. While some like metals and plastics rise at a lesser rate, aggregate demand for imports appears to be on the rise in all sectors. This is an indication of the autoregressive LSTM model's tendency to sustain current patterns of growth while suppressing short-run variability.

```{r}
# =============================================================================
# E) Plot 3: Forecasted UK Imports by Top‑10 Products
# =============================================================================

import_forecast %>%
  filter(product_name %in% top10_import_product_names) %>%
  ggplot(aes(x = year, y = predicted_value, color = product_name)) +
    stat_summary(fun = sum, geom = "line", size = 1) +
    labs(
      title = "Forecasted UK Imports by Top 10 Products",
      x     = "Year",
      y     = "Total Predicted Import Value",
      color = "Product"
    ) +
    theme_minimal()

```

# Export top 10 Countries

Figure X shows projected UK export values to its top 10 major international trading partners from 2020 to 2030. The USA is poised to remain the largest export market, with Germany and the Netherlands following in second and third places. All countries show steady upward trends, reflecting strong and well-founded export growth in the UK's largest trade relationships.
```{r}
# =============================================================================
# D) Plot 2: Forecasted UK Exports to Top‑10 Partner Countries
# =============================================================================

export_forecast %>%
  filter(importer_name %in% top10_export_country_names) %>%
  ggplot(aes(x = year, y = predicted_value, color = importer_name)) +
    stat_summary(fun = sum, geom = "line", size = 1) +
    labs(
      title = "Forecasted UK Exports to Top 10 Countries",
      x     = "Year",
      y     = "Total Predicted Export Value",
      color = "Country"
    ) +
    theme_minimal()

```

# Import top 10 countries
Figure X shows forecast UK import values from its top 10 trading nations from 2020 to 2030. China and Germany remain top suppliers, but all countries—particularly Norway and the USA—have steady growth. The general pattern upwards suggests a growing dependence on key global sources and no evidence of falling import business.
```{r}
# =============================================================================
# F) Plot 4: Forecasted UK Imports from Top‑10 Partner Countries
# =============================================================================

import_forecast %>%
  filter(exporter_name %in% top10_import_country_names) %>%
  ggplot(aes(x = year, y = predicted_value, color = exporter_name)) +
    stat_summary(fun = sum, geom = "line", size = 1) +
    labs(
      title = "Forecasted UK Imports from Top 10 Countries",
      x     = "Year",
      y     = "Total Predicted Import Value",
      color = "Country"
    ) +
    theme_minimal()

```
## Deep learning conclusion

The deep learning models constructed to forecast UK trade flows—separately for exports and imports—produced robust, interpretable results. Leveraging embedded representations for products and countries, together with lag-based numerical features, the two models generalized well over training and validation.

The autoregressive structure, forecasting from close proximate history, was responsible for smooth increasing trends in most goods and partners. Although this makes the model powerful, it diminishes responsiveness to surprise shifts in policy or economics.

Overall, the models supply reasonable high-level forecasts and confirm long-term existing trade imbalances, but their conservative results point to a use best suited for long-range planning rather than minute short-run prediction.

# Final conclusion

This article sought to determine whether or not UK trade—total, by product category, and by destination/origin country—can be properly forecast using existing data-based techniques. Starting with 1995-2023 historical facts, visual observation revealed stable patterns such as the UK's chronic trade deficit, its excessive dependence on a few leading trading partners, and the dominance of high-value industrial goods such as motor vehicles, petroleum, and pharmaceuticals.

Correlation and cluster analysis revealed deeper structure in the data, including co-movements between product categories and regional trade blocs. These were insightful background for the design and interpretation of predictive models.

Two deep learning models were then developed to forecast UK imports and exports through 2030. While the models could replicate long-term directional trends—such as the widening trade gap—their strengths and limitations of autoregressive forecasting were also emphasized. In particular, their reliance on past values and smooth patterned output limit responsiveness to shocks, policy changes, or external disturbances.

In short, deep learning may serve to produce penetrating high-level estimations of macroeconomic trade patterns, especially supplemented with intense exploratory probing. However, they must be perceived as trend benchmarks rather than precision forecasts. They may advance future improvements, possibly by combining it with outside economic indicators, memory models that operate over time, or priors on structures.

# Reference

https://www.cepii.fr/CEPII/en/bdd_modele/bdd_modele_item.asp?id=37

# appendix
All code and supporting documents are available on github in the following perma link
https://github.com/maxaus2002/ASSESSMENT-2-for-MAST7220

Full version of report available on github
Accompanying power point available on github (Mainly contains condesned versions of data ommitted from the condensed report but were present in the full report.)
